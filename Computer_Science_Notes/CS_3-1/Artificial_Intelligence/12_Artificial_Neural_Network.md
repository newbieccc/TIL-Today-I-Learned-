# # 인공지능

## 12. 인공신경망(1)

- 컴퓨터과학과 이병래 교수님

### (1) 인공 신경망의 개념

- 인공 신경망의 구조
    - 인공 신경망이란?
        - 두뇌 속의 신경 구조
            - 약 10억~100억개의 신경세포(뉴런)로 구성
            - 신경세포는 매우 간단한 처리만 담당
            - 각 신경세포는 약 1천~10만 개의 다른 신경세포와 신경연접(synapse)을 통해 연결
            - 신경연접을 지나가는 신호는 신경연접의 특성에 따라 증폭되거나 감쇄되어 전달됨
        - 신경 구조를 모델링하여 지능적 처리에 응용하려고 시도함
            - 인공 신경회로망(Artificial Neural Network, ANN)
        - 인공 신경망의 특성
            - 입력된 내용을 합하고 변환을 하는 간단한 연산기능을 가지고 있는 많은 수의 뉴런으로 구성
            - 각각의 뉴런은 다른 뉴런과 방대한 연결을 유지
            - 수많은 뉴런이 동시에 동작하는 대단위 병렬처리
            - 정보의 저장 : 신경연접의 연결 가중치 벡터를 통해 저장
                - 수많은 뉴런이 서로 연결되는 신경연접에 분산 저장됨
            - 학습능력 : 학습 데이터에 따란 자동적으로 연결 가중치 조정
            - 결함내성 : 일부 뉴런에 고장이 발생해도 전체 시스템의 성능이 급격하게 저하되지 않음
- 인공 신경망 기본 구조
    - 활성함수(activation function)
        - 비선형 특성을 갖는 함수를 사용함
            - 단위 계단함수
                - 함수의 입력이 0보다 작으면 0, 그렇지 않으면 1을 출력함
            - 시그모이드
                - 전 구간에서 미분 가능
                - 자동이득조절 : 작은 입력에 대해서는 이득이 크고, 큰 입력에 대해서는 이득이 작음
            - TanH
                - 전 구간에서 미분 가능
            - ReLU
                - Rectified Linear Unit
    - 연결 형태
        - 흥분성 연결
            - 연결된 뉴런의 활성을 높임
        - 금지 연결
            - 연결된 뉴런의 활성을 낮춤
        - 층내연결
        - 층간연결
        - 피드포워드 연결
        - 순환 연결
- 신경망의 학습
    - 신경망 학습의 개념
        - 학습(learning) : 신경망이 주어진 목적에 맞게 동작할 수 있도록 연결 가중치 w의 값을 결정하는 제반 과정을 의미함
            - 손실함수 정의 (loss function)
            - 하이퍼파라미터(hyperparameter) 설정
            - 신경망의 초기화
            - 훈련(training) : 학습 대상 파라미터의 반복적 업데이트
            - 훈련 결과의 검증(validation)
            - 모델의 테스트(test)
    - 학습 데이터 집합의 활용
        - 학스 데이터의 구성 : 입력 데이터와 레이블(지도학습)
        - 훈련, 검증, 테스트를 위한 데이터 활용
    - 훈련의 진행 단위
        - 개별 학습표본 단위의 파라미터 업데이트
            - 확률적 경사하강법(stochastic gradient descent, SGD)
                - 매 학습 단계에서 학습표본을 무작위로 섞어 훈련을 진행함
        - 배치 학습(batch learning)
            - 학습표본 집합 내의 각각의 표본에 의한 파라미터 변화량을 누적함
            - 전체 학습표본에 대한 변화량의 평균으로 파라미터를 업데이트함
        - 미니배치(mini-batch) SGD
            - 전체 학습표본 집합을 작은 부분집합(미니배치)으로 분할
            - 미니배치 단위의 파라미터 변화량 평균으로 파라미터를 업데이트함

### (2) 퍼셉트론 학습

- 퍼센트론(Perceptron) 모델 개요
    - Frank Rosenblatt - Cornell 대학
        - 1957년 퍼셉트론 학습이론 제시
        - 선형분리가 가능한 입력벡터의 집합에 대한 선형 결정경계를 학습할 수 있음을 입증
    - 뉴런의 기능
        - 입력벡터의 각각의 요소는 연결 가중치를 거쳐 합산되며, 그 결과에 활성함수를 가하여 출력
        - 활성함수 : 단위 계단 함수(unit step function)
    - 지도학습 방식 적용
- 퍼셉트론 학습의 특성
    - 퍼셉트론 학습모델의 성과와 한계
        - 성과 : 학습데이터를 이용하여 반복 학습하면 이를 분리할 수 있는 선형 결정경계를 형성
        - 한계 : 선형분리가 되지 않는 문제는 해결 불가
    - XOR 문제의 해법 : 2단계 처리
        - 1단계 : 동일한 입력에 2개의 퍼셉트론을 구성하여 각각의 출력 u<sub>1</sub>과 u<sub>2</sub>로 새로운 2차원 공간 형성
        - 2단계 : u<sub>1</sub>-u<sub>2</sub> 공간에 사상된 결과를 u<sub>1</sub>과 u<sub>2</sub>를 입력으로 하는 퍼셉트론에 의해 결정경계 형성
